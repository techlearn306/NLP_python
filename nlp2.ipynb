{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# The following is used to install required packages\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_dqLIX9tVm0q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcNaSbNxN-8x",
        "outputId": "14185750-556f-47d5-dd86-66eacfe60ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCGV2DZCODG4",
        "outputId": "e7cc129b-5ae5-4d6d-aff7-330981d43339"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "3XoLdGVnOiSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMnEw4hBVLjW",
        "outputId": "fdeddbb1-569a-4ddd-e2fe-807407dba23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2Grh-G7U7Dq",
        "outputId": "d10be6a0-8a02-4009-8def-f580affc87b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the following is used to tokenizing the sentences ans words"
      ],
      "metadata": {
        "id": "c4D4X6D8V31V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_string = \"\"\"\n",
        "... Muad'Dib learned rapidly because his first training was in how to learn.\n",
        "... And the first lesson of all was the basic trust that he could learn.\n",
        "... It's shocking to find how many people do not believe they can learn,\n",
        "... and how many more believe learning to be difficult.\"\"\""
      ],
      "metadata": {
        "id": "US_YYK7eOrVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(example_string)\n",
        "[\"Muad'Dib learned rapidly because his first training was in how to learn.\",\n",
        "'And the first lesson of all was the basic trust that he could learn.',\n",
        "\"It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beTXaGyaUxFi",
        "outputId": "a075e7de-dab2-4010-fc27-0c34b537a736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Muad'Dib learned rapidly because his first training was in how to learn.\",\n",
              " 'And the first lesson of all was the basic trust that he could learn.',\n",
              " \"It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(example_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jryelXHPUxIi",
        "outputId": "a6cd8e28-6900-4fc0-f5ef-816af77f8196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Muad'Dib\",\n",
              " 'learned',\n",
              " 'rapidly',\n",
              " 'because',\n",
              " 'his',\n",
              " 'first',\n",
              " 'training',\n",
              " 'was',\n",
              " 'in',\n",
              " 'how',\n",
              " 'to',\n",
              " 'learn',\n",
              " '.',\n",
              " 'And',\n",
              " 'the',\n",
              " 'first',\n",
              " 'lesson',\n",
              " 'of',\n",
              " 'all',\n",
              " 'was',\n",
              " 'the',\n",
              " 'basic',\n",
              " 'trust',\n",
              " 'that',\n",
              " 'he',\n",
              " 'could',\n",
              " 'learn',\n",
              " '.',\n",
              " 'It',\n",
              " \"'s\",\n",
              " 'shocking',\n",
              " 'to',\n",
              " 'find',\n",
              " 'how',\n",
              " 'many',\n",
              " 'people',\n",
              " 'do',\n",
              " 'not',\n",
              " 'believe',\n",
              " 'they',\n",
              " 'can',\n",
              " 'learn',\n",
              " ',',\n",
              " 'and',\n",
              " 'how',\n",
              " 'many',\n",
              " 'more',\n",
              " 'believe',\n",
              " 'learning',\n",
              " 'to',\n",
              " 'be',\n",
              " 'difficult',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n"
      ],
      "metadata": {
        "id": "4rOXtl2U1E1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = WordPunctTokenizer()\n",
        "tokenizer.tokenize(\"Let's see how it's working.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBBImOF31HDg",
        "outputId": "ed0cc66b-f896-4187-acc1-387a41b9920f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "text = \"Let's see how it's working.\"\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVm5VuQc1OAC",
        "outputId": "f8d787c7-6146-4bb4-9141-b77b3a4e92e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', 's', 'see', 'how', 'it', 's', 'working']"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is used to filtering and stop words"
      ],
      "metadata": {
        "id": "IVrCFKh1WCYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kq0b_IUZsol",
        "outputId": "36c3e57a-e9b4-402e-9783-0a2285ce1a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "wkP91YAhUxLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "Dju4khNRZ2N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "worf_quote = \"Sir, I protest. I am not a merry man!\""
      ],
      "metadata": {
        "id": "TBIv9QxaUxOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote = word_tokenize(worf_quote)"
      ],
      "metadata": {
        "id": "ChL-iiOLUxRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuRr0z1PUxTz",
        "outputId": "1c29953d-ae03-4142-c90c-ad7dce4b06fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "NASvjVnIUxWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list = []"
      ],
      "metadata": {
        "id": "Zmi70njAUxZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words_in_quote:\n",
        "   if word.casefold() not in stop_words:\n",
        "        filtered_list.append(word)"
      ],
      "metadata": {
        "id": "WLKYb8WpUxdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list = [\n",
        "    word for word in words_in_quote if word.casefold() not in stop_words\n",
        "]"
      ],
      "metadata": {
        "id": "_hujef_5Uxl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0eY1P4fUxog",
        "outputId": "78ed39d7-3a35-411c-c3db-f1c39c8ad66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sir', ',', 'protest', '.', 'merry', 'man', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stemming"
      ],
      "metadata": {
        "id": "PylvzxqYb9XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "qs_QG_9AUxrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "oCaqlX9QUxuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "8h1uehf6Uxw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_for_stemming = \"\"\"\n",
        "The crew of the USS Discovery discovered many discoveries.\n",
        "Discovering is what explorers do.\"\"\""
      ],
      "metadata": {
        "id": "70wFbPLCUxzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(string_for_stemming)"
      ],
      "metadata": {
        "id": "AyH60vO6Ux2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE_VmlZPUx5G",
        "outputId": "fe2665bd-0ee0-4783-dfc5-44a66ac727aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'crew',\n",
              " 'of',\n",
              " 'the',\n",
              " 'USS',\n",
              " 'Discovery',\n",
              " 'discovered',\n",
              " 'many',\n",
              " 'discoveries',\n",
              " '.',\n",
              " 'Discovering',\n",
              " 'is',\n",
              " 'what',\n",
              " 'explorers',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words = [stemmer.stem(word) for word in words]"
      ],
      "metadata": {
        "id": "tO1C7gDpUx79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsF5rf2XUx-_",
        "outputId": "eae77080-c25d-411f-b9d2-628cf308dd35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'crew',\n",
              " 'of',\n",
              " 'the',\n",
              " 'uss',\n",
              " 'discoveri',\n",
              " 'discov',\n",
              " 'mani',\n",
              " 'discoveri',\n",
              " '.',\n",
              " 'discov',\n",
              " 'is',\n",
              " 'what',\n",
              " 'explor',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Create a Porter Stemmer instance\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Example words for stemming\n",
        "words = [\"running\", \"jumps\", \"happily\", \"programming\"]\n",
        "\n",
        "# Apply stemming to each word\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MFeE9Pv3IlK",
        "outputId": "7c5b1637-e1c3-4249-9842-ed4cf74d43e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'jumps', 'happily', 'programming']\n",
            "Stemmed words: ['run', 'jump', 'happili', 'program']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tagging parts of **speech**"
      ],
      "metadata": {
        "id": "RPrCx75Uc1kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "flov8S7nUyBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sagan_quote = \"\"\"\n",
        "If you wish to make an apple pie from scratch,\n",
        "you must first invent the universe.\"\"\""
      ],
      "metadata": {
        "id": "mkzQrfCrcmhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_sagan_quote = word_tokenize(sagan_quote)"
      ],
      "metadata": {
        "id": "wJAFpTg9cmj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_sagan_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFSTc-bScmm0",
        "outputId": "00d82a18-27b0-49f0-9f3b-30868d97cfbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['If',\n",
              " 'you',\n",
              " 'wish',\n",
              " 'to',\n",
              " 'make',\n",
              " 'an',\n",
              " 'apple',\n",
              " 'pie',\n",
              " 'from',\n",
              " 'scratch',\n",
              " ',',\n",
              " 'you',\n",
              " 'must',\n",
              " 'first',\n",
              " 'invent',\n",
              " 'the',\n",
              " 'universe',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "xcbvjRxNddLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OPT2uT7ddOG",
        "outputId": "13827e82-c0c7-437a-db3c-293df7273d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZGhlEqaddQ3",
        "outputId": "14c8cc33-e9c5-46ec-a14e-7242df173dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_sagan_quote = word_tokenize(sagan_quote)"
      ],
      "metadata": {
        "id": "TtcZxpqUddTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.pos_tag(words_in_sagan_quote)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10o4JVDGddWu",
        "outputId": "5a2fb652-1852-4808-eb26-83305a85a879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('If', 'IN'),\n",
              " ('you', 'PRP'),\n",
              " ('wish', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('make', 'VB'),\n",
              " ('an', 'DT'),\n",
              " ('apple', 'NN'),\n",
              " ('pie', 'NN'),\n",
              " ('from', 'IN'),\n",
              " ('scratch', 'NN'),\n",
              " (',', ','),\n",
              " ('you', 'PRP'),\n",
              " ('must', 'MD'),\n",
              " ('first', 'VB'),\n",
              " ('invent', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('universe', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizing\n",
        "Now that you’re up to speed on parts of speech, you can circle back to lemmatizing. Like stemming, lemmatizing reduces words to their core meaning, but it will give you a complete English word that makes sense on its own instead of just a fragment of a word like 'discoveri'.\n",
        "\n",
        "Note: A lemma is a word that represents a whole group of words, and that group of words is called a lexeme.\n",
        "\n",
        "For example, if you were to look up the word “blending” in a dictionary, then you’d need to look at the entry for “blend,” but you would find “blending” listed in that entry.\n",
        "\n",
        "In this example, “blend” is the lemma, and “blending” is part of the lexeme. So when you lemmatize a word, you are reducing it to its lemma."
      ],
      "metadata": {
        "id": "jbmIHuKJfONd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "ukQuV4XwUyFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "PkwLL67be9aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"scarves\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mjp2nSuFe9df",
        "outputId": "db126607-6332-4269-ed19-1c6db541a7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'scarf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string_for_lemmatizing = \"The friends of DeSoto love scarves.\""
      ],
      "metadata": {
        "id": "swb3SyKke9gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(string_for_lemmatizing)"
      ],
      "metadata": {
        "id": "1UHQV9rPe9jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TANO0RFOe9mT",
        "outputId": "8e77d14d-35ac-42ef-a41e-cb2e196c1b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'friends', 'of', 'DeSoto', 'love', 'scarves', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "id": "wEZAI9m3e9o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpL9CXDZe9r0",
        "outputId": "606a1814-0d87-4389-eeec-3c487936ed69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sS37CuAQe9u0",
        "outputId": "fd24be1e-b254-4759-b6ef-c5fff2e70110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sbB5HzRee9xr",
        "outputId": "9cd0f5c7-7950-42b7-999d-2dc04f6bb164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import these modules\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIT232pa5dWC",
        "outputId": "77975bdb-13cf-451b-f03a-6904e47c3dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chunking"
      ],
      "metadata": {
        "id": "eaY_ijlGe-Y7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "sample_text=\"\"\"\n",
        "Rama the legend of the Ramayan is the most popular Indian epic.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YP34kO4Qe-bx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aaecc17-c064-4663-fb68-cc040a82e670"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized=nltk.sent_tokenize(sample_text)\n",
        "for i in tokenized:\n",
        "  words=nltk.word_tokenize(i)"
      ],
      "metadata": {
        "id": "Ezguh9Kke-e0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(words)\n",
        "tagged_words=nltk.pos_tag(words)"
      ],
      "metadata": {
        "id": "tVyHa7Mye-hZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # print(tagged_words)\n",
        "chunkGram=r\"\"\"VB: {}\"\"\"\n",
        "chunkParser=nltk.RegexpParser(chunkGram)\n",
        "chunked=chunkParser.parse(tagged_words)\n",
        "print(chunked)"
      ],
      "metadata": {
        "id": "LW3qDCDee-kx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23235b9d-3596-461b-b06e-c5daab094b36"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  Rama/NNP\n",
            "  the/DT\n",
            "  legend/NN\n",
            "  of/IN\n",
            "  the/DT\n",
            "  Ramayan/NNP\n",
            "  is/VBZ\n",
            "  the/DT\n",
            "  most/RBS\n",
            "  popular/JJ\n",
            "  Indian/JJ\n",
            "  epic/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import RegexpParser"
      ],
      "metadata": {
        "id": "9a0guZL_mZHC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize and Tag\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = word_tokenize(text)\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "print(tagged_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBL13IHimfz3",
        "outputId": "200904e1-ea29-4be6-b9d3-4ad1f871582c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Chunking Grammar:A chunking grammar is defined using regular expressions to specify the patterns of POS tags that constitute a chunk.\n",
        "grammar = r\"NP: {<DT>?<JJ>*<NN>}\""
      ],
      "metadata": {
        "id": "xVQa7Wfvmf_l"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a Chunk Parser: A RegexpParser is created using the defined grammar.\n",
        "chunk_parser = RegexpParser(grammar)"
      ],
      "metadata": {
        "id": "-P2KZWKAmgCz"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parse the Tagged Tokens:The parse() method of the chunk parser is used to chunk the tagged tokens.((NP) Noun Phrase).\n",
        "chunked_tokens = chunk_parser.parse(tagged_tokens)\n",
        "print(chunked_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUQYRsIumgGJ",
        "outputId": "457bccf4-6fc0-414c-a30e-ed08426c04ef"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for subtree in chunked_tokens.subtrees():\n",
        "    if subtree.label() == 'NP':\n",
        "        print(subtree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc7oZ6ywnCsD",
        "outputId": "6e344148-d85f-499a-d6bc-c8e1d877d3c0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(NP The/DT quick/JJ brown/NN)\n",
            "(NP fox/NN)\n",
            "(NP the/DT lazy/JJ dog/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Named entities\n",
        "import nltk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "\n",
        "text = \"This is a company located in Cupertino, California. It was founded by Steve Jobs in 1976.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-rOs29EqeHN",
        "outputId": "98111c80-8a51-4219-cf98-cdfff4f66de6"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'company', 'located', 'in', 'Cupertino', ',', 'California', '.', 'It', 'was', 'founded', 'by', 'Steve', 'Jobs', 'in', '1976', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform part-of-speech tagging\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "# Perform named entity recognition using chunking\n",
        "entities = nltk.ne_chunk(tagged_tokens)\n",
        "\n",
        "# Print the entities\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEGQEgHGri4J",
        "outputId": "aa70830b-0a43-4a73-836d-4571d6cabe87"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  This/DT\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  company/NN\n",
            "  located/VBN\n",
            "  in/IN\n",
            "  (GPE Cupertino/NNP)\n",
            "  ,/,\n",
            "  (GPE California/NNP)\n",
            "  ./.\n",
            "  It/PRP\n",
            "  was/VBD\n",
            "  founded/VBN\n",
            "  by/IN\n",
            "  (PERSON Steve/NNP Jobs/NNP)\n",
            "  in/IN\n",
            "  1976/CD\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract specific entity types (example: GPE)\n",
        "for entity in entities:\n",
        "    if isinstance(entity, nltk.tree.Tree):\n",
        "        if entity.label() == 'GPE':\n",
        "            print(entity.leaves()) # Extract the words of the entity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJQSp4zOrtAP",
        "outputId": "4fbd4a59-4e4d-41c5-c2ac-29c6ee2c15c8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Cupertino', 'NNP')]\n",
            "[('California', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AUyfLCK7uC1J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}